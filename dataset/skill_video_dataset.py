import os
import torch
import numpy as np
import pandas as pd
from PIL import Image
from torch.utils.data import Dataset
from torchvision import transforms
from glob import glob
import sys

# Ensure parent directory is in path to import utils
sys.path.append(".")
sys.path.append("..")
from utils import LongRangeSample

class SkillVideoDataset(Dataset):
    def __init__(self, root_dir, is_train, task, split_type='SuperTrialOut', split_index=1, 
                 sampled_timestep_num=32, frames_per_timestep=16, frame_extraction_downsample=1,
                 return_skill_label='global', debug=False, balanced_train_sample=False,
                 noised_train_label=False, train_sample_augment=1, test_sample_augment=1,
                 return_position_masks=False, score_norm_bias=15, score_norm_weight=25):
        
        self.root_dir = root_dir  # Expects ./data/jigsaws
        self.is_train = is_train
        self.task = task
        self.split_type = split_type
        self.split_index = split_index
        self.return_position_masks = return_position_masks
        
        # Training settings
        self.sampled_timestep_num = sampled_timestep_num
        self.frames_per_timestep = frames_per_timestep
        self.sample_mode = 'random' if self.is_train else 'last'
        self.balanced_train_sample = balanced_train_sample
        self.train_sample_augment = train_sample_augment
        self.test_sample_augment = test_sample_augment
        self.noised_train_label = noised_train_label
        
        # Normalization
        self.score_norm_bias = score_norm_bias
        self.score_norm_weight = score_norm_weight

        # Transforms (ImageNet stats)
        #It tells PyTorch exactly how to massage the raw images into a format the math of the neural network can handle.
        self.transform = transforms.Compose([#Composes several transforms together. 
            transforms.CenterCrop((112, 112)),#get center pixels
            transforms.ToTensor(),#1.chanes range((0-255) --> (0,1)) 2. dimension concersiobn (hwc to chw)
            transforms.Normalize([0.43216, 0.394666, 0.37645], [0.22803, 0.22145, 0.216989]),
        ])
        
        # 1. Load Data Info from Preprocessed CSVs
        self.video_info_dict = self._load_data_info()
        
        # 2. Pre-calculate Sampling List
        self.sample_list = self._generate_sample_list()

    def _load_data_info(self):
        """Reads CSVs generated by preprocessor instead of raw text files."""
        video_info_dict = {}
        
        # Determine path to the split CSV generated by data_preprocessor.py
        # Format: splits/2_Out_itr_1 (for split_index 2)
        split_folder_name = f"{self.split_index}_Out_itr_1"
        split_dir = os.path.join(self.root_dir, 'splits', split_folder_name) #root -> split name folder -> split_folder_name
        
        # Fallback to 'labels' root if split specific folder not found
        if not os.path.exists(split_dir):
            if self.split_index == 1:
                # Default split is often stored in root labels/
                split_dir = os.path.join(self.root_dir, 'labels')
            else:
                print(f"Warning: Split folder {split_dir} not found. Trying default labels.")
                split_dir = os.path.join(self.root_dir, 'labels')

        file_name = 'train_labels.csv' if self.is_train else 'test_labels.csv'
        csv_path = os.path.join(split_dir, file_name)
        
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"Label CSV not found at: {csv_path}. \nPlease run data_preprocessor.py first.")
            
        df = pd.read_csv(csv_path)
        
        for _, row in df.iterrows():
            video_name = row['ClipName']
            score = float(row['GRS_Score'])
            
            # Check image folder to get actual frame count
            img_dir = os.path.join(self.root_dir, 'images', video_name)
            if not os.path.exists(img_dir):
                # Skip if images weren't processed for this clip
                continue
                
            # Count jpg files to determine video length
            # Fast glob count
            frames = glob(os.path.join(img_dir, '*.jpg'))
            num_frames = len(frames)
            
            if num_frames == 0:
                continue

            # Load kinematics if needed
            kine_data = None
            if self.return_position_masks:
                kine_path = os.path.join(self.root_dir, 'kinematics', f"{video_name}.npy")
                if os.path.exists(kine_path):
                    kine_data = np.load(kine_path)
            
            # Store info: [num_frames, score, kinematics_data]
            video_info_dict[video_name] = [num_frames, score, kine_data]
            
        if len(video_info_dict) == 0:
            raise RuntimeError(f"No valid clips found in {csv_path}. Check your data/jigsaws/images folder.")
            
        return video_info_dict

    def _generate_sample_list(self):
        sample_list = []
        
        for video_name, info in self.video_info_dict.items():
            num_frames = info[0]
            
            # Ensure we have enough frames for the window
            # Frames are already downsampled, so they are consecutive indices in the folder
            valid_len = num_frames - self.frames_per_timestep + 1
            if valid_len <= 0: continue

            augment_count = self.train_sample_augment if self.is_train else self.test_sample_augment
            
            for idx in range(augment_count):
                if self.is_train:
                    # 'random' sampling for training
                    timesteps = LongRangeSample.long_range_sample(valid_len, self.sampled_timestep_num, 'random')#slicing and selecting frames 
                    #The LongRangeSample function typically works by cutting the video into equal slices (segments). It then picks one random frame from inside each slice.
                else:
                    # 'last' sampling usually for testing to be deterministic
                    timesteps = LongRangeSample.long_range_sample(valid_len, self.sampled_timestep_num, 'last')#last as its testing
                
                sample_list.append([video_name, idx, timesteps])
                #video_name: "Suturing_E001" (Which folder to open).
                #idx: The sample ID (0, 1, 2...). This is mostly for debugging or tracking which augmentation this is.
                #timesteps: The list of 32 specific frame numbers (e.g., [15, 45, 98, ...]),specifce tiem in vdo, that were just calculated.#Just look at everything evenly, and the Neural Network will figure out which of those 32 clips are actually important."
                
        return sample_list

    def __len__(self):
        return len(self.sample_list)

    def __getitem__(self, idx):
        video_name, sample_idx, timesteps = self.sample_list[idx]
        num_frames, score, kine_data = self.video_info_dict[video_name]

        tensors = []
        img_dir = os.path.join(self.root_dir, 'images', video_name)
        
        # Calculate all frame indices needed
        all_fidxs = []
        for t in timesteps:
            # t is the start index of the chunk
            start = max(0, min(t, num_frames - self.frames_per_timestep))
            #Calculates Limit: 100 - 16 = 84.
            #Check: min(95, 84) results in 84.
            #Result: The code ignores your request for 95 and starts at 84 instead. It grabs frames 84â€“100.  
            block_idxs = list(range(start, start + self.frames_per_timestep))
            all_fidxs.extend(block_idxs)#.extend() iterates through its argument to add each item individually.(like adding list to list)

        # Load images
        for fidx in all_fidxs:
            path = os.path.join(img_dir, f"{fidx:05d}.jpg")#It formats the frame index (e.g., 123) into a zero-padded string (00123.jpg) to match the filename.
            try:
                img = Image.open(path).convert('RGB')#forcefully converts i,age to rbg even if its b;ack mnd white
                tensors.append(self.transform(img))
            except Exception as e:
                # Fallback: return black frame if image read fails
                # print(f"Error loading {path}: {e}")
                tensors.append(torch.zeros(3, 112, 112))#it just inserts a black square (torch.zeros) and keeps going.

        video_tensor = torch.stack(tensors) # (T*F) x C x H x W
        #T (Timesteps) 32
        #F (Frames per step) 16
        # Total Images Loaded: 32 * 16 = 512 images.
        #(512,3,112,112)(112 are the center pixels)
        #all of this forms below:
        
        # Reshape to expected format: L x C x F x H x W (if F > 1) or L x C x H x W (if F=1)
        if self.frames_per_timestep > 1:
            # Reshape to [Timesteps, Frames, C, H, W]
            video_tensor = video_tensor.view(self.sampled_timestep_num, self.frames_per_timestep, 3, 112, 112)
            # Permute to [Timesteps, C, Frames, H, W]
            video_tensor = video_tensor.permute(0, 2, 1, 3, 4)#The permute function takes the new order of indices for the dimensions
            
        # Add Noise to Label (Data Augmentation)
        #By randomly changing the score slightly (e.g., turning a 15 into a 16), you teach the AI to treat the score as a General Estimate rather than an absolute fact
        #prevents overfitting to absolute scores that is-> "This specific video frame equals exactly 15.000
        if self.noised_train_label and self.is_train:
            noise = np.random.choice([-1, 0, 0, 1])
            score = max(min(score + noise, 30), 0)

        # Normalize Score
        if self.score_norm_weight != 0:
            score = (score - self.score_norm_bias) / self.score_norm_weight

        # Return (add dummy mask if requested to prevent crashes)
        if self.return_position_masks:
             dummy_mask = torch.zeros(self.sampled_timestep_num, 14, 14)
             return video_tensor, score, video_name, sample_idx, dummy_mask# dummy zero mask thats is no attention mechanism
        
        return video_tensor, score, video_name, sample_idx